{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Cluster computing Challenge: how do we split work across machines? How do we deal with server failure?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Ex1: Hash Table**\n",
    "![img1](https://raw.githubusercontent.com/hetianch/IntroToBigDataApacheSpark/master/img1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'Do': 1, 'like': 1, 'Sam': 2, 'I': 2, 'eggs': 1, 'am': 2, 'Green': 1, 'you': 1, 'ham': 1}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "raw_string = \"I am Sam Sam I am Do you like Green eggs and ham?\"\n",
    "trans_string = ''.join([ch for ch in raw_string if ch not in exclude])\n",
    "list_string = trans_string.split(\" \")\n",
    "\n",
    "dic = {}\n",
    "for str in list_string:\n",
    "\tdic[str]= dic[str] +1 if str in dic.keys() else 1 \n",
    "print dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img2](https://raw.githubusercontent.com/hetianch/IntroToBigDataApacheSpark/master/img2.png)\n",
    "![img3](https://raw.githubusercontent.com/hetianch/IntroToBigDataApacheSpark/master/img3.png)\n",
    "###**challenge of divide and conqure**###\n",
    "When using divide and conquer, you have to consider the network and data locality because moving data between machines is expensive. Even with a low per-machine failure rate, using many machines means that several will fail per day. As machines age, they may fail in ways that cause slow performance (e.g., a failing disk drive that retries each read or write operation multiple times before successfully completing).\n",
    "![img4](https://raw.githubusercontent.com/hetianch/IntroToBigDataApacheSpark/master/img4.png)\n",
    "![img5](https://raw.githubusercontent.com/hetianch/IntroToBigDataApacheSpark/master/img5.png)\n",
    "sort by sending words to different machines based on occurance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##**How to deal with failure and slow task**##\n",
    "The machine that runs a very slow task might be about to fail. Map Reduce deals with failures and slow tasks by re-launching the tasks on other machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Apache Spark Motivation**#\n",
    "* Original Map Reduce involved many I/O operaiton which is slow\n",
    "* Cost of memmory drops, so that we can keep more data in-memmory instead of writing it to disk slowly.\n",
    "![img7](https://raw.githubusercontent.com/hetianch/IntroToBigDataApacheSpark/master/img7.png)\n",
    "only read in from disk for the first query then store data in memmory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img6](https://raw.githubusercontent.com/hetianch/IntroToBigDataApacheSpark/master/img6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**Spark is oftern faster than a traditional MapReduce because: **###\n",
    "* Results do not need to be written to disk\n",
    "* Results do not need to be serialized (converted into a format that can be stored on disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#**Spark Example**#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/cs100/lab1/shakespeare.txt MapPartitionsRDD[121] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create RDD \n",
    "## from list\n",
    "data = [1,2,3,3,4,5]\n",
    "rDD = sc.parallelize(data,4) #create 4 partitions of data\n",
    "rDD\n",
    "## from file\n",
    "distFile = sc.textFile(\"data/cs100/lab1/shakespeare.txt\",4)\n",
    "distFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[267] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform RDD\n",
    "rDD.map(lambda x: x*2)  #[1,4,9,9,16,25]\n",
    "rDD.filter(lambda x: x%2==0) #[2,4]\n",
    "rDD.distinct() #[1,2,3,4,5]\n",
    "\n",
    "rDD.map(lambda x:[x,x+5]) #[[1,6],[2,7],...]\n",
    "rDD.flatMap(lambda x:[x,x+5]) #[1,6,2,7,...]\n",
    "\n",
    "##comments = distFile.filter(isComment) # not sure why isComment is not defined\n",
    "## key-value transformations\n",
    "rdd = sc.parallelize([(1,2),(3,4),(3,6)])\n",
    "rdd.reduceByKey(lambda a,b:a+b).collect() # the operation a+b is for values with the same key\n",
    "\n",
    "rdd2 = sc.parallelize([(1,'a'),(2,'c'),(1,'b')])\n",
    "rdd2.sortByKey()\n",
    "rdd2.groupByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122395"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get data out of RDD\n",
    "rDD = sc.parallelize([5,4,1,2,3])\n",
    "rDD.reduce(lambda a,b:a*b)\n",
    "rDD.take(2)\n",
    "rDD.collect()\n",
    "rDD.takeOrdered(3,lambda s: -1*s) #reverse order\n",
    "distFile.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122395"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cache\n",
    "distFile = sc.textFile(\"data/cs100/lab1/shakespeare.txt\",4)\n",
    "distFile.cache()\n",
    "distFile.count() # runs from memmory since data are cached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shared Variables-- Broadcast Variables (efficiently send large, read-only value to all workers)\n",
    "broadcastVar=sc.broadcast([1,2,3])\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shared Variables-- Accumulators (aggregate values from workers back to driver)\n",
    "accum = sc.accumulator(0) #initialized to 0\n",
    "rdd =sc.parallelize([1,2,3,4])\n",
    "def f(x):\n",
    "    global accum\n",
    "    accum +=x\n",
    "rdd.foreach(f)\n",
    "accum.value\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Broadcast Variable Example: country code lookup for HAM radio call sings\n",
    "#broadcast a list of call sign prefixes to all workers\n",
    "signPrefixes=sc.broadcast(loadCallSignTable()) \n",
    "def processSignCount(sign_count,signPrefixes):\n",
    "    country = lookupCountry(sign_count[0],signPrefixes.value)\n",
    "    count = sign_count[1]\n",
    "    return (country,count)\n",
    "\n",
    "countryContactCounts = (contactCounts.map(processSignCount).reduceByKey((lambda x,y:x_y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#**Data management**#  \n",
    "##**1. pySpark and pandas DataFrame**##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Spark DataFrame to Pands\n",
    "pandas_df = spark_df.toPandas()\n",
    "#Spark DataFrame from Pandas\n",
    "spark_df = context.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**2. pySpark and SQL**##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, 2)), ('a', (1, 3))]\n",
      "[('a', (1, 2)), ('a', (1, 3)), ('b', (4, None))]\n",
      "[('a', (2, 1)), ('a', (3, 1)), ('b', (None, 4))]\n",
      "[('a', (1, 2)), ('c', (None, 3)), ('b', (4, None))]\n"
     ]
    }
   ],
   "source": [
    "#X.join(Y) #inner join by key\n",
    "x = sc.parallelize([(\"a\",1),(\"b\",4)])\n",
    "y = sc.parallelize([(\"a\",2),(\"a\",3)])\n",
    "print x.join(y).collect()\n",
    "\n",
    "#X.leftOuterJoin(Y)\n",
    "print x.leftOuterJoin(y).collect()\n",
    "\n",
    "#Y.leftOuterJoin(X)\n",
    "print y.rightOuterJoin(x).collect()\n",
    "\n",
    "#x.fullOuterJoin(Y)\n",
    "x = sc.parallelize([(\"a\",1),(\"b\",4)])\n",
    "y = sc.parallelize([(\"a\",2),(\"c\",3)])\n",
    "print x.fullOuterJoin(y).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
